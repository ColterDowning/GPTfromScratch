{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOOOOOOH boy we made a GPT and had our fun, but now we need to dive into the world of tokenization, head first. Welcome to hell =)\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "- Why can't LLM spell words? Tokenization.\n",
    "- Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.\n",
    "- Why is LLM worse at non-English languages (e.g. Japanese)? Tokenization.\n",
    "- Why is LLM bad at simple arithmetic? Tokenization.\n",
    "- Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.\n",
    "- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? Tokenization.\n",
    "- What is this weird warning I get about a \"trailing whitespace\"? Tokenization.\n",
    "- Why the LLM break if I ask it about \"SolidGoldMagikarp\"? Tokenization.\n",
    "- Why should I prefer to use YAML over JSON with LLMs? Tokenization.\n",
    "- Why is LLM not actually end-to-end language modeling? Tokenization.\n",
    "- What is the real root of suffering? Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "127 + 677 = 804\n",
    "1275 + 6773 = 8041\n",
    "\n",
    "Egg.\n",
    "I have an Egg.\n",
    "egg.\n",
    "EGG.\n",
    "\n",
    "만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (hello in Korean!)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"안녕하세요 👋 (hello in Korean!)\"\n",
    "\n",
    "# How do we feed this text into Python? A string is defined as immutable sequences of Unicode code points.\n",
    "# Immutable means a state that cannot be modified after it's created. Making a string means there is no going back.\n",
    "# Any operation in Python that modifies the string actually just makes a new one.\n",
    "# Unicode codepoints are defined by the Unicode consortium according to the Unicode standard, which is ~150k characters,\n",
    "# what the characters look like, and what integers represent those characters.\n",
    "# We can get the integer representation of a character in Python by using ord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"안녕하세요 👋 (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool, so each charater has an integer defined to it. Why don't we just use those integers as our tokens\n",
    "# and not even build a tokenizer? \n",
    "# 1. Our vocabulary would be super long (~150k)\n",
    "# 2. The Unicode standard is changing over time\n",
    "\n",
    "# To solve this, we will turn to encodings. The Unicode consortium defines 3 different encodings: UTF-8,\n",
    "# UTF-16, and UTF-32. These take unicode text and translate into binary data, or byte streams. We will\n",
    "# use the UTF-8 encoding (its backwards compatable to ASCI encoding and the 16/32 encodings are more wasteful).\n",
    "\n",
    "list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\")) # gives the raw bytes of the string according to utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would be great to directly feed these byte streams into the transformer, but the transformer architecture\n",
    "# doesn't allow for it. There is some work being done for a tokenization free transformer, but it is in\n",
    "# the early stages.\n",
    "# Instead, we are going to use the byte pair encoding algorithm =) It generally works as follows:\n",
    "# Suppose you have the following bytes to be encoded (11 bytes with a vocab of 4)\n",
    "# aaabdaaabac\n",
    "# Identify the byte pairs that happen the most often and replace with a byte that is not used in the data (Z)\n",
    "# Now our bytes turn into\n",
    "# ZabdZabac (9 bytes with a vocab of 5)\n",
    "# We can repeat this process, replacing 'ab' with Y\n",
    "# ZYdZYac (7 bytes with vocab of 6)\n",
    "# We can now replace \"XY\" with \"X\" to get\n",
    "# XdXac (5 bytes with vocab of 7)\n",
    "# This is the furthest this data can be compressed, since there are no more pairs that occur more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "# Lets get to the implementation\n",
    "\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # converts to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print('length:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 32)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_common_pair(tokens):\n",
    "    pair_counts = Counter()\n",
    "    # Iterate through all adjacent pairs\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i + 1])\n",
    "        pair_counts[pair] += 1\n",
    "    \n",
    "    # Find the most common pair\n",
    "    if pair_counts:\n",
    "        most_common_pair, _ = pair_counts.most_common(1)[0]\n",
    "        return most_common_pair\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(find_most_common_pair(tokens))  # Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
